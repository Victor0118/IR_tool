{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_score(fn_qrels=\"qrels_all.txt\", prediction=\"score.txt\"):\n",
    "    cmd = \"/bin/sh run_eval.sh {} {}\".format(prediction, fn_qrels)\n",
    "    pargs = shlex.split(cmd)\n",
    "    p = subprocess.Popen(pargs, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    pout, perr = p.communicate()\n",
    "    print(\"running {}\".format(cmd))\n",
    "    if len(pout) != 0:\n",
    "        print(pout.decode('utf-8'))\n",
    "    else:\n",
    "        print(perr.decode('utf-8'))\n",
    "    if sys.version_info[0] < 3:\n",
    "        lines = pout.split('\\n')\n",
    "    else:\n",
    "        lines = pout.split(b'\\n')\n",
    "    Map = float(lines[0].strip().split()[-1])\n",
    "    Mrr = float(lines[1].strip().split()[-1])\n",
    "    P30 = float(lines[2].strip().split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running /bin/sh run_eval.sh prediction_2011_trec.txt qrels_all.txt\n",
      "map                   \tall\t0.3838\n",
      "recip_rank            \tall\t0.7588\n",
      "P_30                  \tall\t0.4381\n",
      "\n",
      "running /bin/sh run_eval.sh prediction_2012_trec.txt qrels_all.txt\n",
      "map                   \tall\t0.2313\n",
      "recip_rank            \tall\t0.6140\n",
      "P_30                  \tall\t0.3571\n",
      "\n",
      "running /bin/sh run_eval.sh prediction_2013_trec.txt qrels_all.txt\n",
      "map                   \tall\t0.2655\n",
      "recip_rank            \tall\t0.7952\n",
      "P_30                  \tall\t0.4767\n",
      "\n",
      "running /bin/sh run_eval.sh prediction_2014_trec.txt qrels_all.txt\n",
      "map                   \tall\t0.4108\n",
      "recip_rank            \tall\t0.8295\n",
      "P_30                  \tall\t0.6406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"train_2011\", \"test_2011\", \"train_2013\", \"test_2013\"]\n",
    "years = [\"2011\", \"2012\", \"2013\", \"2014\"]\n",
    "for dataset, year in zip(datasets, years):\n",
    "    f = open(\"prediction_{}.txt\".format(year))\n",
    "    f2 = open(\"prediction_{}_trec.txt\".format(year), \"w\")\n",
    "    fn = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(dataset)\n",
    "    f3 = open(os.path.join(fn, \"id.txt\"))\n",
    "    for l, l2 in zip(f, f3):\n",
    "        score = l[:-1]\n",
    "        qid, iternum, docno, aid, undefined, run_id = l2[:-1].split()\n",
    "        f2.write(\"{} Q0 {} 0 {} lucene4lm\\n\".format(qid, docno, score))\n",
    "\n",
    "    cal_score(prediction=\"prediction_{}_trec.txt\".format(year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
